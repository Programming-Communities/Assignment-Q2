{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+VLSPmRXmN7XcQDdel4dl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Programming-Communities/Assignment-Q2/blob/main/chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aeOkU8jFANtV",
        "outputId": "93383ba6-b8b8-43e7-c6dd-a1f2d5391188"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyCz32mcYjTf8MfU_JlCzF0A-32f73OY6Qc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "\n",
        "genai.configure(api_key=\"Google Api key add karni ha aap nay pass\")\n",
        "\n",
        "model_name = \"gemini-1.5-flash\"\n",
        "model = genai.GenerativeModel(model_name)\n",
        "\n",
        "response = model.generate_content(\"one issue i use google colab but api ki store and use how can possbile\")\n",
        "\n",
        "print(response.text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mOG1MdKqNNd0",
        "outputId": "c3115d13-e022-43d9-b555-c1c11aed1d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## OpenAI Chat Completion API Parameters Explained\n",
            "\n",
            "This document explains the key parameters of the OpenAI Chat Completion API, drawing from the official documentation and practical examples.\n",
            "\n",
            "**1. Messages:**\n",
            "\n",
            "This parameter is an array of objects, each representing a message in the conversation.  Each message object has a `role` (e.g., \"system\", \"user\", \"assistant\") and a `content` (the text of the message).  The API uses this history to understand the context and generate a relevant response.  For example:\n",
            "\n",
            "```json\n",
            "[\n",
            "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
            "  {\"role\": \"user\", \"content\": \"What's the weather like today?\"},\n",
            "  {\"role\": \"assistant\", \"content\": \"I'm sorry, I don't have access to real-time information, like weather.\"}\n",
            "]\n",
            "```\n",
            "\n",
            "The `messages` parameter is crucial for maintaining conversational context and ensuring coherent responses across multiple turns.\n",
            "\n",
            "\n",
            "**2. Model:**\n",
            "\n",
            "This parameter specifies the language model to use for generating the completion.  Different models have varying capabilities and strengths (e.g., `gpt-3.5-turbo`, `gpt-4`).  Choosing the appropriate model depends on the task, desired performance, and cost considerations.  For instance, `gpt-4` generally offers higher quality responses but is more expensive than `gpt-3.5-turbo`.\n",
            "\n",
            "\n",
            "**3. Max Completion Tokens:**\n",
            "\n",
            "This parameter limits the length of the generated response.  It specifies the maximum number of tokens the model can produce.  A token is roughly equivalent to a word or a part of a word.  Limiting this parameter controls the cost and prevents excessively long outputs.  For example, setting `max_tokens` to 50 might result in a shorter, more concise answer than setting it to 200.\n",
            "\n",
            "\n",
            "**4. n:**\n",
            "\n",
            "This parameter specifies how many different completions to generate for a single prompt.  Setting `n` to 2 will produce two distinct responses. This is useful for exploring diverse options or comparing different outputs.  Note that increasing `n` increases the computational cost.\n",
            "\n",
            "\n",
            "**5. Stream:**\n",
            "\n",
            "This boolean parameter enables streaming responses.  When set to `true`, the API sends back partial responses as they are generated, allowing for real-time feedback and progress updates. This is beneficial for interactive applications where immediate feedback is important.  Setting it to `false` (default) will only return the complete response at once.\n",
            "\n",
            "\n",
            "**6. Temperature:**\n",
            "\n",
            "This parameter controls the randomness of the model's output.  A lower temperature (e.g., 0) produces more focused and deterministic responses, while a higher temperature (e.g., 1) results in more creative and diverse, but potentially less coherent, outputs.  A temperature of 0.7 is often a good starting point.\n",
            "\n",
            "\n",
            "**7. Top_p (nucleus sampling):**\n",
            "\n",
            "Similar to temperature, `top_p` controls the randomness of the model's output but in a different way.  It considers the most likely tokens whose cumulative probability exceeds the `top_p` value.  For instance, `top_p = 0.8` means the model only considers tokens whose cumulative probability adds up to at least 80%.  This method often produces more focused and coherent responses than temperature alone.\n",
            "\n",
            "\n",
            "**8. Tools:**\n",
            "\n",
            "This parameter (available with certain models) allows you to specify external tools the model can use to enhance its capabilities. This could include things like code execution, web search, or access to databases.  This enables the model to perform complex tasks that go beyond simple text generation. For example, you could instruct the model to use a web search tool to answer a factual question.\n",
            "\n",
            "\n",
            "**Interaction of Parameters:**\n",
            "\n",
            "These parameters interact in complex ways. For instance, using a high temperature with a low `top_p` might produce surprising results, while a low temperature with a high `top_p` might generate more predictable outputs.  The choice of model also significantly impacts the effectiveness of other parameters. Experimentation is key to finding the optimal parameter settings for a specific application.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}